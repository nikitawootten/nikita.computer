[{"content":"Opinions expressed on this site are my own and do not necessarily reflect those of my employer\n","date":null,"permalink":"/","section":"","summary":"Opinions expressed on this site are my own and do not necessarily reflect those of my employer","title":""},{"content":"","date":null,"permalink":"/tags/makefile/","section":"Tags","summary":"","title":"Makefile"},{"content":"","date":null,"permalink":"/tags/nix/","section":"Tags","summary":"","title":"Nix"},{"content":" For a list of topics that posts have been tagged with, click here ","date":null,"permalink":"/posts/","section":"Posts","summary":" For a list of topics that posts have been tagged with, click here ","title":"Posts"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/tips/","section":"Tags","summary":"","title":"Tips"},{"content":" I have a secret: I adore Makefiles. I\u0026rsquo;ll admit, the syntax is a bit arcane, and if you don\u0026rsquo;t know what you\u0026rsquo;re doing you can create some really insidious bugs, but once things are set up you can really improve the developer experience on your projects likely without requiring developers to install any additional tools. In this post I\u0026rsquo;d like to share some tips I\u0026rsquo;ve gathered for making your Makefiles more effective.\nThis post assumes that you have surface knowledge of Makefiles. If you\u0026rsquo;d like to learn more about Makefiles, check out the resources in the conclusion. Tip: Automatically document your Makefiles #Picture this, you clone a random project off the internet. The project\u0026rsquo;s documentation instructs you to run make help, and to your delight you are greeted with a nicely formatted list of targets and their purpose. This happened to me when playing around with a project called sbomnix and since discovering it I\u0026rsquo;ve begun including a \u0026ldquo;self-documenting\u0026rdquo; help target to all of my projects:\n.PHONY: help help: ## Show this help message @grep --no-filename -E \u0026#39;^[a-zA-Z_-]+:.*?##.*$$\u0026#39; $(MAKEFILE_LIST) | awk \u0026#39;BEGIN { \\ FS = \u0026#34;:.*?## \u0026#34;; \\ printf \u0026#34;\\033[1m%-30s\\033[0m %s\\n\u0026#34;, \u0026#34;TARGET\u0026#34;, \u0026#34;DESCRIPTION\u0026#34; \\ } \\ { printf \u0026#34;\\033[32m%-30s\\033[0m %s\\n\u0026#34;, $$1, $$2 }\u0026#39; Now, running make help with a Makefile with this target present produces a nice list of targets and their description. Any target annotated with ## \u0026lt;comment\u0026gt; will show up (including the help target). For example, here is the output of make help on the Makefile used to build this very site:\nThis snippet is modified from this blog post. I modified it to support targets within Makefile includes and to add the header. Other variations of this idea exist, choose one that suits you best, or make your own!\nYou might also want to consider making help your default goal:\n# Run the help goal when the user runs `make` .DEFAULT_GOAL: help Tip: Parallelize your Makefile #For larger projects with a lot of moving parts, you could potentially drastically speed up your build by running some targets in parallel. Faster builds means better developer productivity, and also much better CI performance! GitHub Actions gives you a 2-core CPU by default, so there is performance you are potentially leaving on the table!\nThankfully, running Make operations in parallel is trivially easy, just add a --jobs \u0026lt;n\u0026gt; flag (where n is the limit to the number of jobs a Makefile can run at once, usually the number of cores your machine has).\n# Run the specified target with up to \u0026lt;n\u0026gt; jobs in parallel make \u0026lt;target\u0026gt; --jobs \u0026lt;n\u0026gt; For more details, check out the \u0026ldquo;Parallel Execution\u0026rdquo; section of the GNU Make Manual.\nTip: Augment common Makefile operations with canned recipes #Canned recipes are useful when several targets have a lot of similarities. Recipes can also improve the readability of a Makefile.\nFor more details, see the \u0026ldquo;Canned Recipes\u0026rdquo; section of the GNU Make Manual.\nExample: Run commands within a Nix shell #Canned recipes are particularly useful for reducing the amount of repeated code, which can improve readability and reduce the possibility of mistakes.\nFor example, I house my NixOS configurations in a repository with a Makefile for common operations (updating, building, etc). Some of these commands run in a special Nix Shell environment, allowing me to guarantee that the person running the commands has the correct dependencies.\nI initially wrote my Makefile like this:\n.PHONY: help test update switch-home build-home test: ## Test flake outputs with \u0026#34;nix flake check\u0026#34; nix-shell shell.nix --command \u0026#39;nix flake check\u0026#39; update: ## Update \u0026#34;flake.lock\u0026#34; nix-shell shell.nix --command \u0026#39;nix flake update\u0026#39; switch-home: ## Switch local home-manager config nix-shell shell.nix --command \u0026#39;home-manager switch --flake .\u0026#39; build-home: ## Build local home-manager config nix-shell shell.nix --command \u0026#39;home-manager build --flake .\u0026#39; # ... more targets excluded for brevity Using canned recipes I reduced it to this:\n# Run command in nix-shell for maximum reproducibility (idiot [me] proofing) define IN_NIXSHELL nix-shell shell.nix --command \u0026#39;$1\u0026#39; endef .PHONY: help test update switch-home build-home test: ## Test flake outputs with \u0026#34;nix flake check\u0026#34; $(call IN_NIXSHELL,nix flake check) update: ## Update \u0026#34;flake.lock\u0026#34; $(call IN_NIXSHELL,nix flake update) switch-home: ## Switch local home-manager config $(call IN_NIXSHELL,home-manager switch --flake .) build-home: ## Build local home-manager config $(call IN_NIXSHELL,home-manager build --flake .) Example: A simple For-Each recipe #The following canned recipe creates a simple \u0026ldquo;for-each\u0026rdquo; loop:\n# $(call FOREACH,\u0026lt;item variable\u0026gt;,\u0026lt;items list\u0026gt;,\u0026lt;command\u0026gt;) define FOREACH for $1 in $2; do {\\ $3 ;\\ } done endef friends:=bob alice .PHONY: greet greet: # Note that shell variables must be escaped with a double-$ $(call FOREACH,friend,$(friends),echo \u0026#34;hello $$friend\u0026#34;) This method does not play nicely with parallelization, since the loop runs serially. In a lot of cases a better approach is to use patterns and wildcard rules Running this makefile produces the output:\n$ make for friend in bob alice; do { echo \u0026#34;hello $friend\u0026#34; ; } done hello bob hello alice Example: Extending the For-Each recipe for multi-Makefile monorepos #Sometimes you\u0026rsquo;ll have repositories with a lot of moving parts, including several project each complete with their own Makefiles. Wouldn\u0026rsquo;t it be great to have a single top-level Makefile that can run the test target for each project? Fortunately extending the For-Each recipe to do so is trivial:\n# $(call FOREACH_MAKE,\u0026lt;target\u0026gt;,\u0026lt;directories list\u0026gt;) # Run a Makefile target for each directory, requiring each directory to # have a given target define FOREACH_MAKE @echo Running makefile target \\\u0026#39;$1\\\u0026#39; on all subdirectory makefiles @$(call FOREACH,dir,$2,$(MAKE) -C $$dir $1) endef # For all Makefiles matched by the wildcard, extract the directory dirs:=$(dir $(wildcard ./*/Makefile)) .PHONY: test test: ## Run all tests $(call FOREACH_MAKE,$@,$(dirs)) Running make test now runs each sub-directory\u0026rsquo;s test target.\nIn some cases you might want to run a target on each Makefile, ignoring Makefiles that do not define one. For example, say some sub-projects have a clean target, and others don\u0026rsquo;t. The following canned recipe allows you to run make clean only on directories that have a clean target:\n# $(call FOREACH_MAKE_OPTIONAL,\u0026lt;target\u0026gt;,\u0026lt;directories list\u0026gt;) # Run a Makefile target for each directory, skipping directories whose Makefile does not contain a rule define FOREACH_MAKE_OPTIONAL @echo Running makefile target \\\u0026#39;$1\\\u0026#39; on all subdirectory makefiles that contain the rule @$(call FOREACH,dir,$2,$(MAKE) -C $$dir -n $1 \u0026amp;\u0026gt; /dev/null \u0026amp;\u0026amp; $(MAKE) -C $$dir $1 || echo \u0026#34;Makefile target \u0026#39;$1\u0026#39; does not exist in \u0026#34;$$dir\u0026#34;. Continuing...\u0026#34;) endef dirs:=$(dir $(wildcard ./*/Makefile)) .PHONY: clean clean: ## Remove any generated test or build artifacts $(call FOREACH_MAKE_OPTIONAL,$@,$(dirs)) Tip: Use Makefile include for multi-Makefile projects #Using some of the tips you\u0026rsquo;ve gathered in this post, you may have accrued quite a bit of boilerplate now replicated in multiple Makefiles within the same repository. Maybe now you\u0026rsquo;ve added a \u0026ldquo;help\u0026rdquo; target to each Makefile and one or two shared canned recipes. This is not very DRY, we can do better!\nFortunately, Makefile includes make it simple to consolidate shared roles, recipes, and variables.\nMakefile inclusions are not \u0026ldquo;namespaced\u0026rdquo;, so beware of clashing target and variable names. # Include the contents of the Makefile ../shared/common.mk include ../shared/common.mk Conclusion #If you have any Makefile tips that you\u0026rsquo;d like to share, feel free to leave a comment below or contact me.\nIf you\u0026rsquo;d like to learn more about Makefiles, check out some of the links below:\nThis really gentle introduction to Makefiles is great to send to team members looking to get started. The GNU Make Manual is an excellent reference. I find something interesting each time I read through it. Self-Documented Makefile is the excellent blog whose \u0026ldquo;help\u0026rdquo; target I modified above. ","date":"7 July 2023","permalink":"/posts/makefile-tips/","section":"Posts","summary":"I have a secret: I adore Makefiles.","title":"Tips for using Makefiles in your projects"},{"content":"","date":null,"permalink":"/tags/flakes/","section":"Tags","summary":"","title":"Flakes"},{"content":"","date":null,"permalink":"/tags/nodejs/","section":"Tags","summary":"","title":"Nodejs"},{"content":"I recently rediscovered Nix, the confusingly named trifecta of language, package manager, and build system (oh, and an operating system but at least it has a slightly distinct name!), after getting increasingly frustrated with the state of configuration management.\nColleagues of mine frequently get burned with builds failing because of mismatched versions of some specific package on their machine, or some specific flag that should have been enabled in some configuration file that wasn\u0026rsquo;t documented anywhere. Worse, I get burned managing multiple personal and work machines that always have slight differences between them. I\u0026rsquo;ve tried to solve the latter before with Ansible but little differences still pile up and making my Ansible configuration work on different operating systems and distributions becomes its own chore. This isn\u0026rsquo;t a post about me solving these issues specifically, but Nix might be part of the solution which has me very excited.\nThis isn\u0026rsquo;t the first time Nix has made me excited. Around a year ago I found out about Nix, got super excited about it and even replaced my primary dev machine\u0026rsquo;s operating system with NixOS only to be inundated with incomplete documentation, weird compromises (mostly caused by fundamental misunderstandings over what Nix is), and a community split over the adoption of Flakes. I quickly got lost and my productivity plummeted while I tried to figure out how to even get VSCode working properly. To put a long story short, I put the cart before the horse and fell for the hype before even realizing what the hype was about.\nThis time is different. I\u0026rsquo;ve decided to take things slowly this time and really understand Nix before fully committing to it. Part of that commitment will be documenting my journey and all the pain-points I come across.\nIn this post I\u0026rsquo;d like to share how I wrote my first overlay package in Nix, and some general tips I\u0026rsquo;ve gathered surrounding overlays and Flakes.\nThe Application in question #The application in question is OSCAL-deep-diff, a simple node application I built for work that compares large JSON documents.\nDisclaimer: Although I am the author and current maintainer of OSCAL-deep-diff, and while I work on this project as part of my job, this is not an official package endorsed by my organization.\nPackaging an application with Nix (especially with flakes) provides some really cool properties:\nYou can reuse the package in other places easily (including other flakes). If packaged properly, you can run the package from anywhere Nix is installed using nix run. Packaging the application #Nix\u0026rsquo;s build system is a complex patchwork of bash scripts and confusion as explained in this incredibly helpful article by Julia Evans. Thankfully Nix provides a lot of helpers that make packaging really simple. Unfortunately, figuring out how to use these abstractions is another matter, as not a lot of examples exist online and documentation is sparse. I managed to get my package working by dissecting examples like this.\nHopefully this writeup will serve as a good starting point for people trying to package similar applications built on top of the NPM ecosystem.\nIt all starts with a package.json #OSCAL-deep-diff, like many Typescript-based CLI applications that leverage the NPM ecosystem, is just a bunch of Typescript code that links to other Javascript code that makes up its many dependencies:\nOSCAL-deep-diff\u0026rsquo;s dependency graph, generated via npmgraph.js Lucky for me, all the \u0026ldquo;building\u0026rdquo; (compiling Typescript into Javascript) has already been done and all that my Nix derivation has to do is download all of the code and its dependencies, and stick it in the right place.\nI\u0026rsquo;m having Yarn do all of the heavy lifting of downloading the built Javascript code and resolve all of its dependencies.\nNOTE: I chose to use Yarn instead of NPM here purely because it was the easiest for me to get working, your mileage may vary.\nIn my package directory I can create a package.json:\n// packages/oscal-deep-diff/package.json { \u0026#34;dependencies\u0026#34;: { // My application is a single dependency // The version defined here will be the packaged application\u0026#39;s version \u0026#34;@oscal/oscal-deep-diff\u0026#34;: \u0026#34;1.0.0\u0026#34; }, // None of this matters, but yarn gets really angry if you omit it and things will break \u0026#34;name\u0026#34;: \u0026#34;oscal-deep-diff\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;NIST-PD-fallback\u0026#34; } Running yarn produces a lockfile containing the versions of all my package\u0026rsquo;s dependencies, which can then be transformed into a Nix expression using yarn2nix.\nIn Nix this is as easy as running:\n# Run the command `yarn2nix` in an environment with the package `yarn2nix` $ nix-shell -p yarn2nix --command yarn2nix I now have a package.json, yarn.lock, and yarn.nix, but how do I go about actually doing something useful with it?\nCreating the derivation #My Nix derivation needs to:\nDownload all Javascript dependencies (the node_modules/ folder) to the output folder. Create a script that invokes my application\u0026rsquo;s starting point. Step 1 is fairly easy using the mkYarnModules helper. The following Nix expression produces a derivation that downloads all our dependencies to a node_modules/ folder:\n# assuming the package name (pname), version, and nixpkgs as an input pkgs.mkYarnModules { inherit pname version; packageJSON = ./package.json; yarnLock = ./yarn.lock; yarnNix = ./yarn.nix; } This fragment can be consumed in our final derivation (see the deps variable):\n# packages/oscal-deep-diff/default.nix { pkgs ? (import \u0026lt;nixpkgs\u0026gt; {}).pkgs }: let pname = \u0026#34;oscal-deep-diff\u0026#34;; # extract the version from package.json (ensuring these never get out of sync) version = (builtins.fromJSON (builtins.readFile ./package.json)).dependencies.\u0026#34;@oscal/oscal-deep-diff\u0026#34;; # grab our dependencies deps = pkgs.mkYarnModules { inherit pname version; packageJSON = ./package.json; yarnLock = ./yarn.lock; yarnNix = ./yarn.nix; }; in pkgs.stdenv.mkDerivation { inherit pname version; # No build dependencies, all work has been done for you already by mkYarnModules nativeBuildInputs = with pkgs; [ ]; buildInputs = with pkgs; [ ]; # Grab the dependencies from the above mkYarnModules derivation configurePhase = \u0026#39;\u0026#39; mkdir -p $out/bin ln -s ${deps}/node_modules $out \u0026#39;\u0026#39;; # Write a script to the output folder that invokes the entrypoint of the application installPhase = \u0026#39;\u0026#39; cat \u0026lt;\u0026lt;EOF \u0026gt; $out/bin/oscal-deep-diff #!${pkgs.nodejs}/bin/node require(\u0026#39;$out/node_modules/@oscal/oscal-deep-diff/lib/cli/cli.js\u0026#39;); EOF chmod a+x $out/bin/oscal-deep-diff \u0026#39;\u0026#39;; # Skip the unpack step (mkDerivation will complain otherwise) dontUnpack = true; } In the configure phase the derivation creates a symbolic link to the node_modules/ folder created from the deps variable (the mkYarnModules call)/\nIn the install phase the derivation produces a script that invokes the entrypoint of the application. Also notice that the shebang of the script points to ${pkgs.nodejs}/bin/node, which is the version of node packaged by the pkgs.nodejs derivation.\nTesting the derivation #Building the derivation is as simple as running nix-build, which should produce an output folder ./result containing our packaged script in ./result/bin and all dependencies in ./result/node_modules.\nUsing the derivation from within a Flake #Creating an overlay package #Nix overlays are simple patterns that allow you to override your nixpkgs variable in order to add more packages or customize existing ones. As of now I\u0026rsquo;ve only had to do the former, thankfully it\u0026rsquo;s pretty simple to do!\nI started with a overlay that looked like this:\n# packages/default.nix final: prev: { # Import \u0026#34;default.nix\u0026#34; from the \u0026#34;oscal-deep-diff\u0026#34; directory oscal-deep-diff = prev.callPackage ./oscal-deep-diff { } } This module can now be passed in as an argument wherever your import nixpkgs.\nSharing package versions with flake.lock #Currently when we build our derivation with nix-build, the version of nixpkgs used by modules like mkYarnModules and mkDerivation is defined by the system channel, not the version defined in the flake. This inconsistency is subtle but easily avoidable.\nWhat if we used Nix\u0026rsquo;s default argument operator to allow pkgs to be passed in when invoked through a flake, but if invoked through nix-build use the version of nixpkgs listed in the flake\u0026rsquo;s lockfile?\nIt would look something like this:\n# packages/oscal-deep-diff/default.nix (fragment) { pkgs ? let # grab the lockfile and pull out the entry for `nixpkgs` lock = (builtins.fromJSON (builtins.readFile ../../flake.lock)).nodes.nixpkgs.locked; nixpkgs = fetchTarball { url = \u0026#34;https://github.com/nixos/nixpkgs/archive/${lock.rev}.tar.gz\u0026#34;; sha256 = lock.narHash; }; in import nixpkgs { } , ... }: # ... pkgs.stdenv.mkDerivation {} # ... I use this pattern everywhere. It makes it very easy to create dev shells with mkShell that share a Flake\u0026rsquo;s environment even when Flakes aren\u0026rsquo;t enabled on the system.\nBonus: Wrapping common operations in a makefile #I want to make operations like regenerating the yarn.nix file as painless as possible. I do not want to have to remember to install yarn, yarn2nix, and run a specific set of commands to update the package version.\nThankfully, Nix makes this really easy using a dev shell.\nFirst, in my oscal-deep-diff package directory I create a shell.nix containing all my dependencies:\n# packages/oscal-deep-diff/shell.nix { pkgs ? let lock = (builtins.fromJSON (builtins.readFile ../../flake.lock)).nodes.nixpkgs.locked; nixpkgs = fetchTarball { url = \u0026#34;https://github.com/nixos/nixpkgs/archive/${lock.rev}.tar.gz\u0026#34;; sha256 = lock.narHash; }; in import nixpkgs { } , ... }: pkgs.mkShell { packages = with pkgs; [ nix yarn yarn2nix ]; } I can enter this environment interactively with nix-shell shell.nix, but why do so when we can automate all operations using make:\n# packages/oscal-deep-diff/Makefile SHELL:=/usr/bin/env bash IN_NIXSHELL:=nix-shell shell.nix --command .PHONY: build genlock clean build: genlock $(IN_NIXSHELL) \u0026#39;nix-build\u0026#39; genlock: yarn.lock yarn.nix yarn.lock: package.json $(IN_NIXSHELL) \u0026#39;yarn install --mode update-lockfile\u0026#39; rm -fr node_modules yarn.nix: yarn.lock $(IN_NIXSHELL) \u0026#39;yarn2nix \u0026gt; yarn.nix\u0026#39; clean: rm -fr result yarn.* Notice, that all targets are running inside the Nix shell environment defined earlier. That means that if I want to update the package, all I have to do is run make, even if I\u0026rsquo;m not in an environment that has yarn installed.\nConclusion #I hope this little retrospective helps you navigate Nix a little easier!\n","date":"8 April 2023","permalink":"/posts/packaging-node-applications-in-nix/","section":"Posts","summary":"I recently rediscovered Nix, the confusingly named trifecta of language, package manager, and build system (oh, and an operating system but at least it has a slightly distinct name!","title":"Packaging Node Applications in Nix using Yarn2Nix"},{"content":"I really like Drew DeVault\u0026rsquo;s Openring, an elegant utility that generates links to blogs that I follow under my posts (scroll to the end of this article, you may find something you like!). In this post I\u0026rsquo;d like to walk you through how I set up Openring with my Congo themed Hugo blog.\nConfiguring Openring #Openring takes in a Go templated HTML file (see the official example) as well as the feeds you want to display as arguments, and produces a filled out template as a result. I first wrote a simple script that stuffed a file containing a list of RSS feeds into the correct arguments needed to run openring:\n# openring.sh #!/usr/bin/env bash FEEDLIST=config/openring/feeds.txt INPUT_TEMPLATE=config/openring/openring_template.html OUTPUT=layouts/partials/openring.html readarray -t FEEDS \u0026lt; $FEEDLIST # populated below OPENRING_ARGS=\u0026#34;\u0026#34; for FEED in \u0026#34;${FEEDS[@]}\u0026#34; do OPENRING_ARGS=\u0026#34;$OPENRING_ARGS -s $FEED\u0026#34; done openring $OPENRING_ARGS \u0026lt; $INPUT_TEMPLATE \u0026gt; $OUTPUT This snippet reads a list of feeds and a template living in config/openring/ and spits out a Hugo partial template containing the populated list of articles in the layouts/partials directory.\nMy Openring template is a tweaked version of the example provided on the Openring repo. My tweaks center around playing nicely with the Congo theme which uses Tailwind for styling.\n\u0026lt;section class=\u0026#34;webring\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Articles from blogs I follow around the net:\u0026lt;/h3\u0026gt; \u0026lt;section class=\u0026#34;flex flex-wrap\u0026#34;\u0026gt; {{range .Articles}} \u0026lt;div class=\u0026#34;article flex flex-col m-1 p-1 bg-neutral-300 dark:bg-neutral-600\u0026#34;\u0026gt; \u0026lt;h4 class=\u0026#34;m-0\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{.Link}}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt; \u0026lt;/h4\u0026gt; \u0026lt;p class=\u0026#34;summary\u0026#34;\u0026gt;{{.Summary}}\u0026lt;/p\u0026gt; \u0026lt;small\u0026gt; via \u0026lt;a href=\u0026#34;{{.SourceLink}}\u0026#34;\u0026gt;{{.SourceTitle}}\u0026lt;/a\u0026gt; \u0026lt;/small\u0026gt; \u0026lt;small\u0026gt;{{.Date | datef \u0026#34;January 2, 2006\u0026#34;}}\u0026lt;/small\u0026gt; \u0026lt;/div\u0026gt; {{end}} \u0026lt;/section\u0026gt; \u0026lt;p class=\u0026#34;text-sm text-neutral-500 dark:text-neutral-400 text-right\u0026#34;\u0026gt; Generated by \u0026lt;a href=\u0026#34;https://git.sr.ht/~sircmpwn/openring\u0026#34;\u0026gt;openring\u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;/section\u0026gt; {{/* For the bits I couldn\u0026#39;t figure out how to represent in Tailwind */}} \u0026lt;style\u0026gt; .webring .article { flex: 1 1 0; min-width: 10rem; } .webring .summary { flex: 1 1 0; font-size: 0.8rem; } \u0026lt;/style\u0026gt; Loading in the generated Openring template #Depending on your theme, the process of injecting your Openring template will differ. I\u0026rsquo;m using Congo, which specifies that a custom partial can be injected into the footer of all pages simply by naming it layouts/partials/extend-footer.html. This isn\u0026rsquo;t perfect, as I only want my Openring partial to be loaded on articles, excluding pages like my homepage, but luckily Hugo\u0026rsquo;s template syntax makes this easy to fix by using the .IsPage field:\n{{/* layouts/partials/extend-footer.html */}} {{/* Don\u0026#39;t load the partial if it doesn\u0026#39;t exist */}} {{ if templates.Exists \u0026#34;partials/openring.html\u0026#34; }} {{ if .IsPage }} {{/* Only display at the bottom of articles */}} \u0026lt;div class=\u0026#34;mt-6\u0026#34;\u0026gt; {{ partial \u0026#34;openring.html\u0026#34; . }} \u0026lt;/div\u0026gt; {{ end }} {{ end }} Now running the previously described script openring.sh followed by hugo serve should produce the results we\u0026rsquo;re looking for!\nTying it all together #Next, I made a simple makefile that pipelines openring.sh before running any relevant Hugo commands:\n# Makefile openring: ./openring.sh serve: openring hugo serve -p 8080 build: openring hugo build-prod: openring hugo --minify Now in my deployment instead of running hugo --minify I simply run hugo build-prod.\nIf you\u0026rsquo;d like to see the final state of my Hugo site after making these changes, check here as a reference.\n","date":"3 December 2022","permalink":"/posts/hugo-openring/","section":"Posts","summary":"I really like Drew DeVault\u0026rsquo;s Openring, an elegant utility that generates links to blogs that I follow under my posts (scroll to the end of this article, you may find something you like!","title":"Configuring my Congo themed Hugo blog to use Openring"},{"content":"","date":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":"","date":null,"permalink":"/tags/meta/","section":"Tags","summary":"","title":"Meta"},{"content":"","date":null,"permalink":"/tags/tutorial/","section":"Tags","summary":"","title":"Tutorial"},{"content":"","date":null,"permalink":"/tags/cloudflare/","section":"Tags","summary":"","title":"Cloudflare"},{"content":"","date":null,"permalink":"/tags/dns/","section":"Tags","summary":"","title":"Dns"},{"content":"","date":null,"permalink":"/tags/homelab/","section":"Tags","summary":"","title":"Homelab"},{"content":"","date":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes"},{"content":"So, your ISP won\u0026rsquo;t give you a static IP address for your burgeoning homelab\u0026rsquo;s network? Tired of telling friends what your IP address is every time they want to access your Minecraft server? Dynamic DNS is an extremely simple solution to that problem. The gist is, if you can\u0026rsquo;t rely on a static IP address, just have your servers periodically tell a third party where they\u0026rsquo;re located.\nIn this post I\u0026rsquo;d like to share my exploration of Kubernetes and the CloudFlare API.\nFirst of all I\u0026rsquo;d like to emphasize that there are definitely easier ways to do this. Many consumer (even ISP provided) routers can connect to services like NoIp and DynDNS without any hassle, and it\u0026rsquo;s trivial to spin up services like DDClient in a Docker container or even packaged through your favorite Linux distribution. I am running this in my Kubernetes cluster mostly out of curiosity.\nI\u0026rsquo;m sharing this post to show how simple it is to run jobs on a Kubernetes cluster using tools that you\u0026rsquo;re most likely already familiar with (simple bash scripts can go a long way!).\nEnter the Cloudflare API #You could use DuckDNS, Google Cloud DNS, AWS Route 53, or really any service that offers you the ability to update DNS records programmatically. I chose CloudFlare because I already have a few domains managed by them.\nLuckily, Cloudflare provides an easy to use API with documentation and examples on how to update a DNS record already provided:\n# PUT https://api.cloudflare.com/client/v4/zones/{zone_identifier}/dns_records/{identifier} curl --request PUT \\ --url https://api.cloudflare.com/client/v4/zones/zone_identifier/dns_records/identifier \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;X-Auth-Email: \u0026#39; \\ --data \u0026#39;{ \u0026#34;content\u0026#34;: \u0026#34;198.51.100.4\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;example.com\u0026#34;, \u0026#34;proxied\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;Domain verification record\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;owner:dns-team\u0026#34; ], \u0026#34;ttl\u0026#34;: 3600 }\u0026#39; So like 70% of the work is already done for us, great! (unfortunately their example is not perfect, and led me to quite a bit of debugging)\nLet\u0026rsquo;s break down their example:\nWe need to make a PUT request to https://api.cloudflare.com/client/v4/zones/{zone_identifier}/dns_records/{identifier}, filling in values for zone_identifier and identifier\nzone_identifier seems to be a special ID given to each domain.\nidentifier is a bit more mysterious. It seems that an identifier is given to each DNS entry, but CloudFlare\u0026rsquo;s dashboard unhelpfully does not tell you what the record identifiers are.\nTo get around this, you can either make an API request to Cloudflare that lists your DNS records for a domain, or you can try updating a record manually with your browser\u0026rsquo;s network tab open to intercept requests. I chose the ladder.\nWe\u0026rsquo;ll need to authenticate our requests.\nI didn\u0026rsquo;t have luck getting Cloudflare\u0026rsquo;s key-based authentication working, but their token-based authentication worked without any problems.\nFor token-based authentication we just provide a bearer token in the Authorization header.\nThe body of the request is fairly simple:\nWe provide the name of the record we\u0026rsquo;d like to update, the type of record and the IP address we\u0026rsquo;d like to set it to The record can be tagged with metadata through the comment and tags fields We can specify a TTL, or Time to Live to control how frequently client DNS caches invalidate (the value they\u0026rsquo;ve provided here is fine) How do we even know what our public IP address is? #Funny enough, the easiest way to find your public IP address is to ask someone else:\nIP_ADDRESS=$(curl https://domains.google.com/checkip) Our simple Bash script #If you throw everything we\u0026rsquo;ve learned so far at the screen you\u0026rsquo;ll probably arrive at a bash script that looks something like this:\nIP=$(curl $CHECK_IP) curl --request PUT --url https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$RECORD_ID \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#34;Authorization: Bearer $API_KEY\u0026#34; \\ --data \u0026#34;{ \\\u0026#34;content\\\u0026#34;: \\\u0026#34;$IP\\\u0026#34;, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;$NAME\\\u0026#34;, \\\u0026#34;proxied\\\u0026#34;: $PROXIED, \\\u0026#34;ttl\\\u0026#34;: $TTL, \\\u0026#34;type\\\u0026#34;: \\\u0026#34;$TYPE\\\u0026#34; }\u0026#34; Fill in the appropriate environment variables and with luck you\u0026rsquo;ve witnessed your DNS become dynamic.\nNow, we could stop here and throw this script in a cronjob, or a systemd service unit, but if you\u0026rsquo;re like me you already have a Kubernetes cluster laying around, so you might as well use it to manage things right?\nEnter Kubernetes #This script involves three simple Kubernetes resources:\nA ConfigMap resource containing our simple bash script. A Secret resource containing the sensitive environment variables that the script relies on. A CronJob resource, which fittingly creates Job resources on a schedule. The ConfigMap resource #ConfigMaps are one of the easier Kubernetes resources to understand. Fundamentally they just store maps (key-value pairs) of data that can be consumed by other resources.\nIn this case, I am using the ConfigMap to store the script that I derived in the last section. This ConfigMap is meant to be mounted onto a container as files, but ConfigMaps can be used in other ways like injecting environment variables into a container.\n# cloudflare-ddns-cronjob.yaml apiVersion: v1 kind: ConfigMap metadata: name: cloudflare-ddns-script-configmap data: run.sh: | IP=$(curl $CHECK_IP) curl --request PUT --url https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$RECORD_ID \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#34;Authorization: Bearer $API_KEY\u0026#34; \\ --data \u0026#34;{ \\\u0026#34;content\\\u0026#34;: \\\u0026#34;$IP\\\u0026#34;, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;$NAME\\\u0026#34;, \\\u0026#34;proxied\\\u0026#34;: $PROXIED, \\\u0026#34;ttl\\\u0026#34;: $TTL, \\\u0026#34;type\\\u0026#34;: \\\u0026#34;$TYPE\\\u0026#34; }\u0026#34; The Secret resource #Kubernetes Secrets are very similar in concept to a ConfigMap, but they are intended to be used to store sensitive variables.\nNote: Secrets are not any more secure then a ConfigMap by default, but with some hardening (encryption at rest \u0026amp; access control with RBAC, retrieval through the K8s API instead of as environment variables) they can be made much more secure. Snyk\u0026rsquo;s article on the subject is a great introduction.\nIn this case I am using the secrets to inject environment variables needed by the script to function:\n# cloudflare-ddns-cronjob.yaml ... --- apiVersion: v1 kind: Secret metadata: name: cloudflare-ddns-secret stringData: ZONE_ID: \u0026#34;\u0026#34; # redacted RECORD_ID: \u0026#34;\u0026#34; # redacted NAME: \u0026#34;example.com\u0026#34; # redacted TYPE: A PROXIED: true TTL: \u0026#34;300\u0026#34; API_KEY: \u0026#34;\u0026#34; # redacted CHECK_IP: https://domains.google.com/checkip The CronJob resource #The CronJob resource is the most complex of the 3, and to be fair there\u0026rsquo;s a lot going on here:\n# cloudflare-ddns-cronjob.yaml ... --- apiVersion: batch/v1 kind: CronJob metadata: name: cloudflare-ddns-job spec: concurrencyPolicy: Forbid failedJobsHistoryLimit: 5 successfulJobsHistoryLimit: 5 startingDeadlineSeconds: 60 schedule: \u0026#34;*/5 * * * *\u0026#34; jobTemplate: metadata: name: cloudflare-ddns-job spec: activeDeadlineSeconds: 240 backoffLimit: 3 template: metadata: name: cloudflare-ddns-job-pod spec: containers: - name: cloudflare-ddns-job-container image: fedora:36 command: [\u0026#34;bash\u0026#34;, \u0026#34;/scripts/run.sh\u0026#34;] envFrom: - secretRef: name: cloudflare-ddns-secret volumeMounts: - name: script-volume mountPath: /scripts volumes: - name: script-volume configMap: name: cloudflare-ddns-script-configmap restartPolicy: OnFailure The Cron part of a CronJob #From the top, first we define some properties about scheduling these jobs:\nconcurrencyPolicy: Forbid failedJobsHistoryLimit: 5 successfulJobsHistoryLimit: 5 startingDeadlineSeconds: 60 schedule: \u0026#34;*/5 * * * *\u0026#34; Even without additional documentation it\u0026rsquo;s fairly clear what these do.\nWe don\u0026rsquo;t want multiple DDNS updates happening simultaneously, so we forbid concurrency. We don\u0026rsquo;t want to fill up our history with jobs, so we only keep the last 5 working and 5 failed jobs. Finally we set the CronJob to create Jobs on an interval using the cron expression format (in this case we\u0026rsquo;re just saying \u0026ldquo;run every 5 minutes\u0026rdquo;). The Job part of a CronJob #Our CronJob emits a Job on a schedule, but what do jobs look like?\ncontainers: - name: cloudflare-ddns-job-container image: fedora:36 command: [\u0026#34;bash\u0026#34;, \u0026#34;/scripts/run.sh\u0026#34;] envFrom: - secretRef: name: cloudflare-ddns-secret volumeMounts: - name: script-volume mountPath: /scripts volumes: - name: script-volume configMap: name: cloudflare-ddns-script-configmap restartPolicy: OnFailure Our job consists of one container running Fedora 36. It\u0026rsquo;ll run the command /scripts/run.sh, which is injected via the ConfigMap via a volume mount. Environment variables will be passed in using the Secret.\nCreating the resource #Now that we have all of our actors, the last step is to create the resources in Kubernetes:\n# optionally create a namespace for our resource first kubectl create namespace cloudflare-ddns # (remove the -n argument to put it on the default namespace) kubectl apply -n cloudflare-ddns -f cloudflare-ddns-cronjob.yaml If all goes well you should start to see jobs running (and hopefully succeeding) periodically:\nMy Kubernetes dashboard after the Cronjob has run a few times Old jobs are cleaned up according to the failedJobsHistoryLimit and successfulJobsHistoryLimit parameters you have set in the CronJob resource.\nConclusion #In this blog post we\u0026rsquo;ve explored:\nWhy Dynamic DNS is used and how it works How to use services like Cloudflare to roll your own Dynamic DNS Some basic Kubernetes resources and how they fit in your toolbox to get things done The full Kubernetes manifest can be found at this GitHub gist.\n","date":"1 December 2022","permalink":"/posts/simple-ddns-k8s-cronjob/","section":"Posts","summary":"So, your ISP won\u0026rsquo;t give you a static IP address for your burgeoning homelab\u0026rsquo;s network?","title":"Simply managing Dynamic DNS in Kubernetes using the CronJob resource and the CloudFlare API"},{"content":"","date":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible"},{"content":"","date":null,"permalink":"/tags/dotfiles/","section":"Tags","summary":"","title":"Dotfiles"},{"content":"","date":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"Update 1/12/2023: I have broken out my dotfiles utility role into its own repository and I\u0026rsquo;ve listed it on Ansible Galaxy. Check it out!.\nConfiguration management is hard. I first started to get serious about managing my dotfiles when I started college. Before that, I\u0026rsquo;d treat the configuration of my machines as a big ball of mud. You start off with a shiny new system, and as you install more and more software, you start to accumulate these things that effect your workflow in mysterious ways. Every time I\u0026rsquo;d find a weird workaround or neat alias to put in my .bashrc, I\u0026rsquo;d just leave it there to be forgotten the next time I started over with a shiny new system. Worse yet, as I graduated to working with a laptop, a desktop, and even a server, my configuration became a distributed big ball of mud.\nConsolidating and codifying all of my configuration into a single source of truth has helped me immensely in several ways:\nI do not get confused by changes between my machines. I can trust that the same aliases and utilities will be with me wherever I go. I do not have to fear \u0026ldquo;starting over\u0026rdquo;. If I accidentally wipe my laptop, or get a new machine, I can be back to working minutes after installing Linux. I get all the benefits of revision control. If I\u0026rsquo;m tinkering with a configuration file and something breaks, I can tell exactly what I changed and when I changed it. My dotfiles journey #But first, a bit about the things I tried before settling on my current system.\nFirst attempt #My first attempt at managing my dotfiles involved a bash script that precariously symlinked files from my dotfiles repository:\n... # this could be pretty dangerous cp -rfs $(pwd)/dotfiles/. ~/ ... This approach definitely beat having nothing in place, but it still had problems. My laptop and desktop machines at the time had vastly different configurations, including different software, desktop environments, and even different Linux distros.\nI needed an approach that lended itself well to having multiple machines with some distinct configuration.\nSecond attempt: grouping configuration files together and GNU Stow #Stow is a symlink manager that can be used pretty easily to manage dotfiles. With Stow, I could group my configuration files for a given piece of software or a machine into its own directory, and apply it all at once.\nStow improved my dotfiles management workflow a lot. Under this new workflow I had configuration specific to each machine, as well as specific configuration for different pieces of software. I could have separate configuration for i3 or bspwm, without polluting my environment on a given machine with both files if I wasn\u0026rsquo;t planning on having it installed.\nThe problem, is that although my configuration files are managed with Stow, there is a lot more to a running system\u0026rsquo;s state, such as:\nWhat services are enabled and running? What packages are installed? What operating system is installed? I needed a solution that manages all aspects of the state of a given machine.\nIntroducing Ansible #Ansible is a really powerful tool that can be used to automate all sorts of systems.\nAnsible is built on a principle of idempotentcy, meaning if Ansible is run twice, the second run should not break the changes that were made the first time. This is a great fit for dotfiles. As my system evolves, I can commit a change on one system, distribute it to the other machines, and update their configuration without worrying about things breaking.\nOrganizing capabilities into Ansible roles #Like I had with Stow, Ansible allows you to group together reusable pieces of configuration into roles. Under the roles/ directory, I could have specific configurations for a given capability I want that machine to have.\nFor example, I have a Git role that:\nInstalls Git Configures Git Altogether the role looks like this:\n# .dotfiles/roles/git/tasks/main.yaml - name: Install Git ansible.builtin.package: name: - git state: present become: yes - name: Configure Git ansible.builtin.shell: | git config --global user.name \u0026#34;Nikita Wootten\u0026#34; git config --global user.email \u0026lt;REDACTED\u0026gt; git config --global core.editor vim git config --global fetch.prune true git config --global pull.rebase false Note: I omitted some lines that I use to check if the Git configuration changed after being updated. The full configuration is here.\nRoles can also depend on other roles, ensuring for example that the role that the role that sets up my Yubikey/GPG configuration is run before the role that sets up my SSH client configuration.\nThe dotfiles role #Many of my roles depend on a small utility I wrote that mimics Stow with Ansible.\nMy custom dotfiles role (which you can find on Ansible Galaxy) scans a role for configuration files, and symlinks to resulting files to the appropriate location.\nMy ZSH role can then ensure all of my ZSH configuration has made it by invoking the dotfiles role:\n# .dotfiles/roles/zsh/tasks/main.yaml --- - name: Symlink zsh dotfiles include_role: name: nikitawootten.dotfiles System playbooks #At the root of my dotfiles repository I have playbooks set up for each of my machines. Each playbook includes the roles which define the capabilities I need for the machine.\nMy laptop\u0026rsquo;s configuration looks like this:\n# .dotfiles/casper-magi.yaml --- - name: Set up casper-magi hosts: localhost roles: - zsh - docker - ssh-client - git - yubikey - update-script The update-script role #The update-script role is another utility role I wrote which creates a script that can be run to update the machine. This role prevents me from accidentally running the wrong playbook after setting up a machine. On subsequent updates I only have to run dotfiles-update.\nTying it all together #Check out my dotfiles here.\n","date":"29 May 2022","permalink":"/posts/dotfiles/","section":"Posts","summary":"Update 1/12/2023: I have broken out my dotfiles utility role into its own repository and I\u0026rsquo;ve listed it on Ansible Galaxy.","title":"How I manage my dotfiles with Ansible"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]