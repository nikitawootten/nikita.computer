[{"content":"This site is very much a work-in-progress, so check back later, you may find something interesting :)\n","date":"8 April 2023","permalink":"/","section":"","summary":"This site is very much a work-in-progress, so check back later, you may find something interesting :)","title":""},{"content":"","date":"8 April 2023","permalink":"/tags/flakes/","section":"Tags","summary":"","title":"flakes"},{"content":"","date":"8 April 2023","permalink":"/tags/nix/","section":"Tags","summary":"","title":"nix"},{"content":"","date":"8 April 2023","permalink":"/tags/nodejs/","section":"Tags","summary":"","title":"nodejs"},{"content":"I recently rediscovered Nix, the confusingly named trifecta of language, package manager, and build system (oh, and an operating system but at least it has a slightly distinct name!), after getting increasingly frustrated with the state of configuration management.\nColleagues of mine frequently get burned with builds failing because of mismatched versions of some specific package on their machine, or some specific flag that should have been enabled in some configuration file that wasn\u0026rsquo;t documented anywhere. Worse, I get burned managing multiple personal and work machines that always have slight differences between them. I\u0026rsquo;ve tried to solve the latter before with Ansible but little differences still pile up and making my Ansible configuration work on different operating systems and distributions becomes its own chore. This isn\u0026rsquo;t a post about me solving these issues specifically, but Nix might be part of the solution which has me very excited.\nThis isn\u0026rsquo;t the first time Nix has made me excited. Around a year ago I found out about Nix, got super excited about it and even replaced my primary dev machine\u0026rsquo;s operating system with NixOS only to be inundated with incomplete documentation, weird compromises (mostly caused by fundamental misunderstandings over what Nix is), and a community split over the adoption of Flakes. I quickly got lost and my productivity plummeted while I tried to figure out how to even get VSCode working properly. To put a long story short, I put the cart before the horse and fell for the hype before even realizing what the hype was about.\nThis time is different. I\u0026rsquo;ve decided to take things slowly this time and really understand Nix before fully committing to it. Part of that commitment will be documenting my journey and all the pain-points I come across.\nIn this post I\u0026rsquo;d like to share how I wrote my first overlay package in Nix, and some general tips I\u0026rsquo;ve gathered surrounding overlays and Flakes.\nThe Application in question # The application in question is OSCAL-deep-diff, a simple node application I built for work that compares large JSON documents.\nDisclaimer: Although I am the author and current maintainer of OSCAL-deep-diff, and while I work on this project as part of my job, this is not an official package endorsed by my organization.\nPackaging an application with Nix (especially with flakes) provides some really cool properties:\nYou can reuse the package in other places easily (including other flakes). If packaged properly, you can run the package from anywhere Nix is installed using nix run. Packaging the application # Nix\u0026rsquo;s build system is a complex patchwork of bash scripts and confusion as explained in this incredibly helpful article by Julia Evans. Thankfully Nix provides a lot of helpers that make packaging really simple. Unfortunately, figuring out how to use these abstractions is another matter, as not a lot of examples exist online and documentation is sparse. I managed to get my package working by dissecting examples like this.\nHopefully this writeup will serve as a good starting point for people trying to package similar applications built on top of the NPM ecosystem.\nIt all starts with a package.json # OSCAL-deep-diff, like many Typescript-based CLI applications that leverage the NPM ecosystem, is just a bunch of Typescript code that links to other Javascript code that makes up its many dependencies:\nOSCAL-deep-diff\u0026rsquo;s dependency graph, generated via npmgraph.js Lucky for me, all the \u0026ldquo;building\u0026rdquo; (compiling Typescript into Javascript) has already been done and all that my Nix derivation has to do is download all of the code and its dependencies, and stick it in the right place.\nI\u0026rsquo;m having Yarn do all of the heavy lifting of downloading the built Javascript code and resolve all of its dependencies.\nNOTE: I chose to use Yarn instead of NPM here purely because it was the easiest for me to get working, your mileage may vary.\nIn my package directory I can create a package.json:\n// packages/oscal-deep-diff/package.json { \u0026#34;dependencies\u0026#34;: { // My application is a single dependency // The version defined here will be the packaged application\u0026#39;s version \u0026#34;@oscal/oscal-deep-diff\u0026#34;: \u0026#34;1.0.0\u0026#34; }, // None of this matters, but yarn gets really angry if you omit it and things will break \u0026#34;name\u0026#34;: \u0026#34;oscal-deep-diff\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;NIST-PD-fallback\u0026#34; } Running yarn produces a lockfile containing the versions of all my package\u0026rsquo;s dependencies, which can then be transformed into a Nix expression using yarn2nix.\nIn Nix this is as easy as running:\n# Run the command `yarn2nix` in an environment with the package `yarn2nix` $ nix-shell -p yarn2nix --command yarn2nix I now have a package.json, yarn.lock, and yarn.nix, but how do I go about actually doing something useful with it?\nCreating the derivation # Our Nix derivation needs to:\nDownload all Javascript dependencies (our node_modules/ folder) to the output folder. Create a script that invokes our application\u0026rsquo;s starting point. Step 1 is fairly easy using the mkYarnModules helper. The following Nix expression produces a derivation that downloads all our dependencies to a node_modules/ folder:\n# assuming the package name (pname), version, and nixpkgs as an input pkgs.mkYarnModules { inherit pname version; packageJSON = ./package.json; yarnLock = ./yarn.lock; yarnNix = ./yarn.nix; } This fragment can be consumed in our final derivation (see the deps variable):\n# packages/oscal-deep-diff/default.nix { pkgs ? (import \u0026lt;nixpkgs\u0026gt; {}).pkgs }: let pname = \u0026#34;oscal-deep-diff\u0026#34;; # extract the version from package.json (ensuring these never get out of sync) version = (builtins.fromJSON (builtins.readFile ./package.json)).dependencies.\u0026#34;@oscal/oscal-deep-diff\u0026#34;; # grab our dependencies deps = pkgs.mkYarnModules { inherit pname version; packageJSON = ./package.json; yarnLock = ./yarn.lock; yarnNix = ./yarn.nix; }; in pkgs.stdenv.mkDerivation { inherit pname version; # No build dependencies, all work has been done for you already by mkYarnModules nativeBuildInputs = with pkgs; [ ]; buildInputs = with pkgs; [ ]; # 1. Copy the node modules folder from the mkYarnModules derivation # 2. Create a script that invokes oscal-deep-diff\u0026#39;s cli entrypoint installPhase = \u0026#39;\u0026#39; mkdir -p $out/bin cp -r ${deps}/node_modules $out cat \u0026lt;\u0026lt;EOF \u0026gt; $out/bin/oscal-deep-diff #!/usr/bin/env node require(\u0026#39;$out/node_modules/@oscal/oscal-deep-diff/lib/cli/cli.js\u0026#39;); EOF chmod a+x $out/bin/oscal-deep-diff \u0026#39;\u0026#39;; # Skip the unpack step (mkDerivation will complain otherwise) dontUnpack = true; } In our install phase we copy the node_modules/ folder from the mkYarnModules derivation and then produce a script that invokes the entrypoint of the application.\nTesting the derivation # Building the derivation is as simple as running nix-build, which should produce an output folder ./result containing our packaged script in ./result/bin and all dependencies in ./result/node_modules.\nUsing the derivation from within a Flake # Creating an overlay package # Nix overlays are simple patterns that allow you to override your nixpkgs variable in order to add more packages or customize existing ones. As of now I\u0026rsquo;ve only had to do the former, thankfully it\u0026rsquo;s pretty simple to do!\nI started with a overlay that looked like this:\n# packages/default.nix final: prev: { # Import \u0026#34;default.nix\u0026#34; from the \u0026#34;oscal-deep-diff\u0026#34; directory oscal-deep-diff = prev.callPackage ./oscal-deep-diff { } } This module can now be passed in as an argument wherever your import nixpkgs.\nSharing package versions with flake.lock # Currently when we build our derivation with nix-build, the version of nixpkgs used by modules like mkYarnModules and mkDerivation is defined by the system channel, not the version defined in the flake. This inconsistency is subtle but easily avoidable.\nWhat if we used Nix\u0026rsquo;s default argument operator to allow pkgs to be passed in when invoked through a flake, but if invoked through nix-build use the version of nixpkgs listed in the flake\u0026rsquo;s lockfile?\nIt would look something like this:\n# packages/oscal-deep-diff/default.nix (fragment) { pkgs ? let # grab the lockfile and pull out the entry for `nixpkgs` lock = (builtins.fromJSON (builtins.readFile ../../flake.lock)).nodes.nixpkgs.locked; nixpkgs = fetchTarball { url = \u0026#34;https://github.com/nixos/nixpkgs/archive/${lock.rev}.tar.gz\u0026#34;; sha256 = lock.narHash; }; in import nixpkgs { } , ... }: # ... pkgs.stdenv.mkDerivation {} # ... I use this pattern everywhere. It makes it very easy to create dev shells with mkShell that share a Flake\u0026rsquo;s environment even when Flakes aren\u0026rsquo;t enabled on the system.\nBonus: Wrapping common operations in a makefile # I want to make operations like regenerating the yarn.nix file as painless as possible. I do not want to have to remember to install yarn, yarn2nix, and run a specific set of commands to update the package version.\nThankfully, Nix makes this really easy using a dev shell.\nFirst, in my oscal-deep-diff package directory I create a shell.nix containing all my dependencies:\n# packages/oscal-deep-diff/shell.nix { pkgs ? let lock = (builtins.fromJSON (builtins.readFile ../../flake.lock)).nodes.nixpkgs.locked; nixpkgs = fetchTarball { url = \u0026#34;https://github.com/nixos/nixpkgs/archive/${lock.rev}.tar.gz\u0026#34;; sha256 = lock.narHash; }; in import nixpkgs { } , ... }: pkgs.mkShell { packages = with pkgs; [ nix yarn yarn2nix ]; } I can enter this environment interactively with nix-shell shell.nix, but why do so when we can automate all operations using make:\n# packages/oscal-deep-diff/Makefile SHELL:=/usr/bin/env bash IN_NIXSHELL:=nix-shell shell.nix --command .PHONY: build genlock clean build: genlock $(IN_NIXSHELL) \u0026#39;nix-build\u0026#39; genlock: yarn.lock yarn.nix yarn.lock: package.json $(IN_NIXSHELL) \u0026#39;yarn install --mode update-lockfile\u0026#39; rm -fr node_modules yarn.nix: yarn.lock $(IN_NIXSHELL) \u0026#39;yarn2nix \u0026gt; yarn.nix\u0026#39; clean: rm -fr result yarn.* Notice, that all targets are running inside the Nix shell environment defined earlier. That means that if I want to update the package, all I have to do is run make, even if I\u0026rsquo;m not in an environment that has yarn installed.\nConclusion # I hope this little retrospective helps you navigate Nix a little easier!\n","date":"8 April 2023","permalink":"/posts/packaging-node-applications-in-nix/","section":"Posts","summary":"I recently rediscovered Nix, the confusingly named trifecta of language, package manager, and build system (oh, and an operating system but at least it has a slightly distinct name!","title":"Packaging Node Applications in Nix using Yarn2Nix"},{"content":"","date":"8 April 2023","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"8 April 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"I really like Drew DeVault\u0026rsquo;s Openring, an elegant utility that generates links to blogs that I follow under my posts (scroll to the end of this article, you may find something you like!). In this post I\u0026rsquo;d like to walk you through how I set up Openring with my Congo themed Hugo blog.\nConfiguring Openring # Openring takes in a Go templated HTML file (see the official example) as well as the feeds you want to display as arguments, and produces a filled out template as a result. I first wrote a simple script that stuffed a file containing a list of RSS feeds into the correct arguments needed to run openring:\n# openring.sh #!/usr/bin/env bash FEEDLIST=config/openring/feeds.txt INPUT_TEMPLATE=config/openring/openring_template.html OUTPUT=layouts/partials/openring.html readarray -t FEEDS \u0026lt; $FEEDLIST # populated below OPENRING_ARGS=\u0026#34;\u0026#34; for FEED in \u0026#34;${FEEDS[@]}\u0026#34; do OPENRING_ARGS=\u0026#34;$OPENRING_ARGS -s $FEED\u0026#34; done openring $OPENRING_ARGS \u0026lt; $INPUT_TEMPLATE \u0026gt; $OUTPUT This snippet reads a list of feeds and a template living in config/openring/ and spits out a Hugo partial template containing the populated list of articles in the layouts/partials directory.\nMy Openring template is a tweaked version of the example provided on the Openring repo. My tweaks center around playing nicely with the Congo theme which uses Tailwind for styling.\n\u0026lt;section class=\u0026#34;webring\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;Articles from blogs I follow around the net:\u0026lt;/h3\u0026gt; \u0026lt;section class=\u0026#34;flex flex-wrap\u0026#34;\u0026gt; {{range .Articles}} \u0026lt;div class=\u0026#34;article flex flex-col m-1 p-1 bg-neutral-300 dark:bg-neutral-600\u0026#34;\u0026gt; \u0026lt;h4 class=\u0026#34;m-0\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{.Link}}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt; \u0026lt;/h4\u0026gt; \u0026lt;p class=\u0026#34;summary\u0026#34;\u0026gt;{{.Summary}}\u0026lt;/p\u0026gt; \u0026lt;small\u0026gt; via \u0026lt;a href=\u0026#34;{{.SourceLink}}\u0026#34;\u0026gt;{{.SourceTitle}}\u0026lt;/a\u0026gt; \u0026lt;/small\u0026gt; \u0026lt;small\u0026gt;{{.Date | datef \u0026#34;January 2, 2006\u0026#34;}}\u0026lt;/small\u0026gt; \u0026lt;/div\u0026gt; {{end}} \u0026lt;/section\u0026gt; \u0026lt;p class=\u0026#34;text-sm text-neutral-500 dark:text-neutral-400 text-right\u0026#34;\u0026gt; Generated by \u0026lt;a href=\u0026#34;https://git.sr.ht/~sircmpwn/openring\u0026#34;\u0026gt;openring\u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;/section\u0026gt; {{/* For the bits I couldn\u0026#39;t figure out how to represent in Tailwind */}} \u0026lt;style\u0026gt; .webring .article { flex: 1 1 0; min-width: 10rem; } .webring .summary { flex: 1 1 0; font-size: 0.8rem; } \u0026lt;/style\u0026gt; Loading in the generated Openring template # Depending on your theme, the process of injecting your Openring template will differ. I\u0026rsquo;m using Congo, which specifies that a custom partial can be injected into the footer of all pages simply by naming it layouts/partials/extend-footer.html. This isn\u0026rsquo;t perfect, as I only want my Openring partial to be loaded on articles, excluding pages like my homepage, but luckily Hugo\u0026rsquo;s template syntax makes this easy to fix by using the .IsPage field:\n{{/* layouts/partials/extend-footer.html */}} {{/* Don\u0026#39;t load the partial if it doesn\u0026#39;t exist */}} {{ if templates.Exists \u0026#34;partials/openring.html\u0026#34; }} {{ if .IsPage }} {{/* Only display at the bottom of articles */}} \u0026lt;div class=\u0026#34;mt-6\u0026#34;\u0026gt; {{ partial \u0026#34;openring.html\u0026#34; . }} \u0026lt;/div\u0026gt; {{ end }} {{ end }} Now running the previously described script openring.sh followed by hugo serve should produce the results we\u0026rsquo;re looking for!\nTying it all together # Next, I made a simple makefile that pipelines openring.sh before running any relevant Hugo commands:\n# Makefile openring: ./openring.sh serve: openring hugo serve -p 8080 build: openring hugo build-prod: openring hugo --minify Now in my deployment instead of running hugo --minify I simply run hugo build-prod.\nIf you\u0026rsquo;d like to see the final state of my Hugo site after making these changes, check here as a reference.\n","date":"3 December 2022","permalink":"/posts/hugo-openring/","section":"Posts","summary":"I really like Drew DeVault\u0026rsquo;s Openring, an elegant utility that generates links to blogs that I follow under my posts (scroll to the end of this article, you may find something you like!","title":"Configuring my Congo themed Hugo blog to use Openring"},{"content":"","date":"3 December 2022","permalink":"/tags/hugo/","section":"Tags","summary":"","title":"hugo"},{"content":"","date":"3 December 2022","permalink":"/tags/meta/","section":"Tags","summary":"","title":"meta"},{"content":"","date":"3 December 2022","permalink":"/tags/tutorial/","section":"Tags","summary":"","title":"tutorial"},{"content":"","date":"29 May 2022","permalink":"/tags/ansible/","section":"Tags","summary":"","title":"ansible"},{"content":"","date":"29 May 2022","permalink":"/tags/dotfiles/","section":"Tags","summary":"","title":"dotfiles"},{"content":"","date":"29 May 2022","permalink":"/tags/git/","section":"Tags","summary":"","title":"git"},{"content":"Update 1/12/2023: I have broken out my dotfiles utility role into its own repository and I\u0026rsquo;ve listed it on Ansible Galaxy. Check it out!.\nConfiguration management is hard. I first started to get serious about managing my dotfiles when I started college. Before that, I\u0026rsquo;d treat the configuration of my machines as a big ball of mud. You start off with a shiny new system, and as you install more and more software, you start to accumulate these things that effect your workflow in mysterious ways. Every time I\u0026rsquo;d find a weird workaround or neat alias to put in my .bashrc, I\u0026rsquo;d just leave it there to be forgotten the next time I started over with a shiny new system. Worse yet, as I graduated to working with a laptop, a desktop, and even a server, my configuration became a distributed big ball of mud.\nConsolidating and codifying all of my configuration into a single source of truth has helped me immensely in several ways:\nI do not get confused by changes between my machines. I can trust that the same aliases and utilities will be with me wherever I go. I do not have to fear \u0026ldquo;starting over\u0026rdquo;. If I accidentally wipe my laptop, or get a new machine, I can be back to working minutes after installing Linux. I get all the benefits of revision control. If I\u0026rsquo;m tinkering with a configuration file and something breaks, I can tell exactly what I changed and when I changed it. My dotfiles journey # But first, a bit about the things I tried before settling on my current system.\nFirst attempt # My first attempt at managing my dotfiles involved a bash script that precariously symlinked files from my dotfiles repository:\n... # this could be pretty dangerous cp -rfs $(pwd)/dotfiles/. ~/ ... This approach definitely beat having nothing in place, but it still had problems. My laptop and desktop machines at the time had vastly different configurations, including different software, desktop environments, and even different Linux distros.\nI needed an approach that lended itself well to having multiple machines with some distinct configuration.\nSecond attempt: grouping configuration files together and GNU Stow # Stow is a symlink manager that can be used pretty easily to manage dotfiles. With Stow, I could group my configuration files for a given piece of software or a machine into its own directory, and apply it all at once.\nStow improved my dotfiles management workflow a lot. Under this new workflow I had configuration specific to each machine, as well as specific configuration for different pieces of software. I could have separate configuration for i3 or bspwm, without polluting my environment on a given machine with both files if I wasn\u0026rsquo;t planning on having it installed.\nThe problem, is that although my configuration files are managed with Stow, there is a lot more to a running system\u0026rsquo;s state, such as:\nWhat services are enabled and running? What packages are installed? What operating system is installed? I needed a solution that manages all aspects of the state of a given machine.\nIntroducing Ansible # Ansible is a really powerful tool that can be used to automate all sorts of systems.\nAnsible is built on a principle of idempotentcy, meaning if Ansible is run twice, the second run should not break the changes that were made the first time. This is a great fit for dotfiles. As my system evolves, I can commit a change on one system, distribute it to the other machines, and update their configuration without worrying about things breaking.\nOrganizing capabilities into Ansible roles # Like I had with Stow, Ansible allows you to group together reusable pieces of configuration into roles. Under the roles/ directory, I could have specific configurations for a given capability I want that machine to have.\nFor example, I have a Git role that:\nInstalls Git Configures Git Altogether the role looks like this:\n# .dotfiles/roles/git/tasks/main.yaml - name: Install Git ansible.builtin.package: name: - git state: present become: yes - name: Configure Git ansible.builtin.shell: | git config --global user.name \u0026#34;Nikita Wootten\u0026#34; git config --global user.email \u0026lt;REDACTED\u0026gt; git config --global core.editor vim git config --global fetch.prune true git config --global pull.rebase false Note: I omitted some lines that I use to check if the Git configuration changed after being updated. The full configuration is here.\nRoles can also depend on other roles, ensuring for example that the role that the role that sets up my Yubikey/GPG configuration is run before the role that sets up my SSH client configuration.\nThe dotfiles role # Many of my roles depend on a small utility I wrote that mimics Stow with Ansible.\nMy custom dotfiles role (which you can find on Ansible Galaxy) scans a role for configuration files, and symlinks to resulting files to the appropriate location.\nMy ZSH role can then ensure all of my ZSH configuration has made it by invoking the dotfiles role:\n# .dotfiles/roles/zsh/tasks/main.yaml --- - name: Symlink zsh dotfiles include_role: name: nikitawootten.dotfiles System playbooks # At the root of my dotfiles repository I have playbooks set up for each of my machines. Each playbook includes the roles which define the capabilities I need for the machine.\nMy laptop\u0026rsquo;s configuration looks like this:\n# .dotfiles/casper-magi.yaml --- - name: Set up casper-magi hosts: localhost roles: - zsh - docker - ssh-client - git - yubikey - update-script The update-script role # The update-script role is another utility role I wrote which creates a script that can be run to update the machine. This role prevents me from accidentally running the wrong playbook after setting up a machine. On subsequent updates I only have to run dotfiles-update.\nTying it all together # Check out my dotfiles here.\n","date":"29 May 2022","permalink":"/posts/dotfiles/","section":"Posts","summary":"Update 1/12/2023: I have broken out my dotfiles utility role into its own repository and I\u0026rsquo;ve listed it on Ansible Galaxy.","title":"How I manage my dotfiles with Ansible"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]